# Log dir
logs:
  dir: logs/${dataset}_

# name under which the run is stored
config: 1008

# for continuing training put path here and true
cont_training: False
cont_dir: './logs/1008_xview60/2023-11-20 08-46-56'

# mode
mode: train_has
model_type: has_ntu

# Env. setting
ddp: False
gpus: [0]
seed: 1234
num_threads: 16
report_freq: 5
no_progress_bar: False
work_dir: /Users/felixtempel/PycharmProjects/GNN_NAS/logs

# Old SP (True) or new (False)
old_sp: False

# ------ Data settings ------------
# xsub60, xview60, xsub120, xsetup120, kinetics, cp
dataset: xview60
dataset_args:
  layout: ntu # ntu -> 300 frames; kinetics 400 frames
  num_frame: 300
  inputs: JVBA
  transform: True # set to True if dataset == xview60!
  normalize: False
  root_folder: /...
  ntu60_path: /...
  ntu120_path: /...
  filter: True
  filter_order: 8

# Dataloader settings
batch_size: 16
num_workers: 12
pin_memory: True
num_frame: 288

# Controller args
use_baseline:   False
reward_map_fn:  None
train_epochs:   25
warmup_rollouts: 20
warmup_epochs: 0
argmax_epochs:  80
rollouts:       20
eval_interval: 5
max_iter: 50

# policy deletion
policy_deletion: False
policy_updates: 0 # trained argmax students before policies are deleted
policy_threshold: 0.15 # difference to others

# Random Search
random_search: False
random_epochs_half: 25
random_iter: 50

# Replay memory
replay_mem: True
replay_batch: 10
replay_cap: 200
replay_thres: 0.8 # min top1 acc. to append # if ntu --> 0.8; kinetics 0.25

# Early stop # for ntu 6, 0.5, 5; for kinetics 6, 0.2, 5
early_stop: True
early_stop_epoch: 6
early_stop_acc: 0.5
early_stop_no_impr: 5

# PROXY Learning settings
proxy_mode: False
proxy_dir: ""

# Bootstrap test - give indicies for arch and hyper space
bootstrap: False
bootstrap_iter: 1000
bootstrap_alpha: 5.0 # 100-alpha --> 95% CI
bootstrap_retrain: True

bootstrap_arch: [2, 0, 0, 1, 2, 1, 1, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0]
bootstrap_hyper: [2, 2, 0, 0, 0]
bootstrap_path: "./logs/bootstrap_xview60/2023-10-12 13-39-52/argmax_9999/student_model_9999.pth.tar"

# controller settings
controller_lr: 0.001
controller_dir: "/controller_weights/"

# fixed optimizer settings
optimizer_args:
  SGD:
    momentum: 0.9
    nesterov: True
  Adam:
    betas: [0.9,0.99]
    amsgrad: True
  RMSprop:
    centered: True
  AdamW:
    amsgrad: True

# lr scheduler settings
# might increase this
lr_scheduler: True
sched_args:
  warm_up: 10
  start_factor: 0.25
  step_lr: [30, 50, 60, 65, 70]
  gamma: 0.5

# Architect Search Space OLD
arch:
  blocks_in:    [1, 2, 3]
  depth_in:     [1, 2]
  stride_in:    [1, 2, 3, 4, 5]
  blocks_main:  [1, 2]
  depth_main:   [1, 2]
  stride_main:  [1, 2, 3, 4, 5]
  temp_win:     [3, 5, 7]
  graph_dist:   [2, 3, 4]
  expand_ratio: [1.05, 1.1, 1.15]
  reduct_ratio: [1.225, 1.25, 1.275, 1.3, 1.325, 1.35]
  scale_in:     [0.4, 0.6, 0.8]
  scale_main:   [1.2, 1.4, 2.0, 2.5, 3.0]
  act:          ["relu", "relu6", "hardswish", "swish"]
  att_lay:      ['stja', 'ca', 'fa', 'ja', 'pa']
  conv_lay:     ["Basic", "Bottleneck", "Sep", "SG", "V3", "Shuffle"]
  init_lay:     [64, 96, 128, 156]
  drop_prob:    [0.15, 0.2, 0.25, 0.3]

# Architect Search Space NEW
arch_2:
  # common
  init_lay:       [64, 128, 156, 195, 256] # big [156, 195, 256, 320] # small [64, 128, 156, 195, 256]
  act:            ["relu", "relu6", "hardswish", "swish"]
  att_lay:        ["stja", "ca", "fa", "ja", "pa"]
  conv_lay:       ["Basic", "Bottleneck", "Sep", "SG", "V3", "Shuffle"]
  drop_prob:      [0.15, 0.2, 0.25, 0.3]
  # input stream
  blocks_in:      [1, 2, 3] # big [2, 3, 4]  # small [1, 2, 3]
  depth_in:       [1, 2 ,3] # big [3, 4, 5] # small [1, 2 ,3]
  stride_in:      [1, 2, 3]
  scale_in:       [0.4, 0.6, 0.8]
  temp_win_in:    [3, 5, 7]
  graph_dist_in:  [2, 3, 4]
  reduct_ratio_in: [1.225, 1.25, 1.275, 1.3, 1.325, 1.35] # highest ES 1.35
  # main stream
  blocks_main:      [3, 4, 5, 6]  # big [5, 6, 7, 8] # small [3, 4, 5, 6]
  depth_main:       [1, 2, 3, 4]  # big [3, 4, 5] # small [1, 2, 3, 4]
  graph_dist_main:  [7, 9, 11]
  shrinkage_main:   [1, 2, 4, 6]
  residual_main:    [True, False]
  adaptive_main:    [True, False]

# Hyperparamter search space -common
hyper:
  lr:           [0.1, 0.05, 0.01]
  optimizers:   ['SGD', 'Adam', 'AdamW']
  weight_decay: [0.0, 0.01, 0.001, 0.0001]
  momentum:     [0.5, 0.9, 0.99]
  batch_size:   [8, 16, 24]

# --------------------------------------------- Debug settings ---------------------------------------------------
debug: False
debug_argmax_epoch: 1
debug_train_epochs: 1
debug_warmup_rollouts: 5
debug_rollouts: 5
debug_load_small_set: True

# Debug SP old
dev:
  blocks_in:    [2]
  depth_in:     [2]
  stride_in:    [3]
  blocks_main:  [2]
  depth_main:   [3]
  stride_main:  [2]
  temp_win:     [5]
  graph_dist:   [2]
  expand_ratio: [1.35]
  reduct_ratio: [1.2]
  scale_in:     [0.8]
  scale_main:   [2]
  act:          ["swish"]
  att_lay:      ['stja']
  conv_lay:     ["SG"] # "EpSep",
  init_lay:     [64]
  drop_prob:    [0.25]

# Debug SP new
dev_2:
  # common
  init_lay: [128]
  act: ["swish" ]
  att_lay: [ "stja"]
  conv_lay: [ "Sep" ]
  drop_prob: [  0.3 ]
  # input stream
  blocks_in: [  1 ]
  depth_in: [  2 ]
  scale_in:     [0.8, 0.9]
  stride_in: [ 1]
  temp_win_in: [ 5]
  graph_dist_in: [ 3]
  reduct_ratio_in: [ 1.25 ]
  # main stream
  blocks_main: [ 2 ]
  depth_main:       [ 3]
  graph_dist_main: [ 9] # kernel size
  shrinkage_main: [ 4]
  residual_main: [ True ]
  adaptive_main: [ True ]

# Debug Hyper Space new/old
hyper_dev:
  lr:           [0.05, 0.1]
  optimizers:   ['AdamW']
  weight_decay: [0.0001]
  momentum:     [ 0.9]
  batch_size:   [ 16]
